{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"admin/","text":"Landing page #","title":"Introduction"},{"location":"admin/#landing-page","text":"","title":"Landing page"},{"location":"admin/installation/","text":"Installation notes #","title":"Installation"},{"location":"admin/installation/#installation-notes","text":"","title":"Installation notes"},{"location":"admin/project/","text":"Admin Project Guide #","title":"Project"},{"location":"admin/project/#admin-project-guide","text":"","title":"Admin Project Guide"},{"location":"admin/user/","text":"Admin User Guide #","title":"User"},{"location":"admin/user/#admin-user-guide","text":"","title":"Admin User Guide"},{"location":"compute/","text":"Landing page #","title":"Introduction"},{"location":"compute/#landing-page","text":"","title":"Landing page"},{"location":"compute/jobs/","text":"Jobs # All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Apache Flink Docker ( Hopsworks Enterprise only ) Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Manage jobs # You can quickly create a new job by following these steps: From the project menu click Jobs and then the New job button Select a file to be used as the program the job will execute. The file can be selected either from within the project, that means you need to select a file stored in one of the project's datasets, or select a file from your local workstation which will be uploaded to the Resources dataset by default. You can also select Jupyter notebooks as files to be executed. When running a job, Hopsworks will automatically convert the PySpark/Python notebook to a .py file and run it. The notebook is converted every time the job runs, which means changes in the notebook will be picked up by the job without having to update it. Fill in a name for the job. The name can be made of alphanumeric characters, dash or underscore. A job's name is unique across the project, that is no two jobs can have the same name. If you would like to explore more advanced options such as importing a job or setting alerts, then you need to click the Advanced options button shown in the image below and from that page you can configure the options shown in the sections below. New job quickstart New job from scratch # In addition to the options available with the new job quickstart, from this menu you can set default arguments to be given as input to the job when executed. You can overwrite these arguments when running a job by providing input arguments for that particular execution of the job. For example, in the image below the default arguments \"a b c\" will be passed as space-separated arguments to the job. Job default arguments Import an existing job # Instead of creating a new job from scratch, you can create one based on another jobs' configuration. The latter can be exported by using the Export button from the job's Overview page as shown below. This action will download a .json file containing the job's specification which you can then import from the advanced options page. Job export Compute Configuration # You can set a default configuration for each job type (currently only supported for Spark and Python ) at a project scope, from the Project settings - Compute configuration menu option. That enables you to have the job configuration parameters already filled in when creating a job and optionally override them for this particular job instance. Compute configuration page Spark # The image below shows an example of a Spark job advanced configuration page. If set, these configuration properties will override the default project-scoped job configuration properties described in the section above. You can set the following properties for Spark/PySpark jobs: Main class: For Spark jobs, the main class of the application Driver memory Driver virtual cores Executor memory Executor virtual cores Dynamic - Static: Run the Spark job in static or dynamic allocation mode (see official docs for details). Additional archives: List of zip or .tgz files that will be locally accessible by the application Additional jars: List of .jar files to add to the CLASSPATH of the application Additional Python dependencies: List of .py, .zip or .egg files that will be locally accessible by the application Additional files: List of files that will be locally accessible by the application Properties: Optional line-separates properties to be set for the Spark application. For example, to set environment variables for the driver you can set the properties as shown below spark.yarn.appMasterEnv.envvar=value spark.yarn.appMasterEnv.envvar2=value2 Spark job configuration properties Python # The image below shows an example of a Python job advanced configuration page. If set, these configuration properties will override the default project-scoped job configuration properties described in the section above. You can set the following properties for Python jobs: Container memory: The amount of memory in MB to be allocated to the container running the Python program Container cores: The number of cores to be allocated to the container running the Python program Additional files: List of files that will be locally accessible by the application Python job configuration properties You do not have to upload the Python program via the Hopsworks UI to run it. It can be done programmatically from a Python program by using the upload function of the dataset module of the hops Python library To do that, first generate an API key for your project, and then use the project.connect() function of the same library to connect to a project of your Hopsworks cluster and then dataset.upload() . Docker # Docker jobs can currently only be managed from the legacy Hopsworks user interface, see documentation at Docker jobs for details. Apache Flink # Flink jobs can currently only be managed from the legacy Hopsworks user interface, see documentation at Flink jobs for details. Alerts # You can attach Hopsworks alerts on a per-job basis if you want to receive notifications for job-specific events. Firstly, a Hopsworks administrator needs to setup alert channels from the Cluster settings page. Currently supported channels are - email - slack - pagerduty Once you have configured the alert channels, you can proceed by specifying alert receivers from the project's settings page. Receivers are the destinations to which your alerts will be sent to. You can specify global alerts, which means these alerts will be triggered on job events regardless of which specific job generated the alert event. Alternatively, you can override the job alerts by adding alerts to a specific job from the job's configuration page as shown below. The job events alerts are triggered upon are: on job success on job fail on job killed The potential severities these alerts should report are: WARNING INFO CRITICAL Job alerts Edit & Delete # You can edit a job from the Job's overview page. If you would like to quickly create a job based on the configuration of another job, you can use the Make a copy button that will prompt you for a name for the new job and then immediately after create the new job. From the same page, you can also delete a job as shown in the image below. Please note that when deleting a job, Hopsworks will first attempt to gracefully terminate any pending executions of this job. All job logs will remain in the Logs dataset and it is up to the user to clean them up if needed. Job deletion Executions # You can execute a job multiple times concurrently with different input arguments. Each execution has an individual execution id that identifies it. Execution logs are stored under the Logs dataset and are categorized per job type, name, execution(Python)/application(Spark) id. Execute a job # You can execute a job either using the Quick run option from the job's preview page, as shown in the image below, or from the job's overview page. The difference between the two is that the latter will prompt you for input arguments whereas the former will use the default arguments, if any are provided. This page also shows a list of the most recent executions. For a detailed list of executions for the job, you need navigate to the executions page by clicking the view all executions link. Job preview Executions overview By default, a job can have a maximum of 10000 executions. This limit can be increased/lowered by a Hopsworks administrator. Monitoring and Logging # The executions page enables you to filter executions for the job based on the execution's date and state. You can view detailed information about each execution by following the various monitoring and logging links provided with each execution. In particular, you get access to the following dashboards/views for every execution: Spark UI: Applies to Spark/PySpark jobs only, a new tab will open showing the Spark application Web UI (see docs ). RM UI: Applies to Spark/PySpark/Flink jobs only, a new tab will open showing the Apache Hadoop YARN page of the execution Kibana: Applies to Spark/PySpark/Flink jobs only, a new tab will open showing the execution logs in real-time as they are collected from all the containers the execution is distributed at. Logs: Shows the aggregated stdout/stderr logs of the execution. These logs are stored under the Logs dataset. Hopsworks IDE Plugin # It is also possible to work on jobs while developing in your IntelliJ/PyCharm IDE by installing the Hopsworks Plugin from the marketplace. Usage Open the Hopsworks Job Preferences UI for specifying user preferences under Settings -> Tools -> Hopsworks Job Preferences . Input the Hopsworks project preferences and job details you wish to work on. Open a Project and within the Project Explorer, right click on the program ( .jar, .py, .ipynb) you wish to execute as a job on Hopsworks. Different job actions possible are available in the context menu ( Create, Run, Stop, etc.) Actions Create: Create or update job as specified in Hopsworks Job Preferences Run: Uploads the program first to the HDFS path as specified and runs job Stop: Stops a job Delete: Deletes a job Job Execution Status / Job Execution Logs: Get the job status or logs respectively. You have the option of retrieving a particular job execution by specifying the execution id in the 'Hopsworks Job Preferences' UI, otherwise default is the last execution for the job name specified. Working with jobs from Hopsworks IntelliJ/PyCharm plugin","title":"Jobs"},{"location":"compute/jobs/#jobs","text":"All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Apache Flink Docker ( Hopsworks Enterprise only ) Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with.","title":"Jobs"},{"location":"compute/jobs/#manage-jobs","text":"You can quickly create a new job by following these steps: From the project menu click Jobs and then the New job button Select a file to be used as the program the job will execute. The file can be selected either from within the project, that means you need to select a file stored in one of the project's datasets, or select a file from your local workstation which will be uploaded to the Resources dataset by default. You can also select Jupyter notebooks as files to be executed. When running a job, Hopsworks will automatically convert the PySpark/Python notebook to a .py file and run it. The notebook is converted every time the job runs, which means changes in the notebook will be picked up by the job without having to update it. Fill in a name for the job. The name can be made of alphanumeric characters, dash or underscore. A job's name is unique across the project, that is no two jobs can have the same name. If you would like to explore more advanced options such as importing a job or setting alerts, then you need to click the Advanced options button shown in the image below and from that page you can configure the options shown in the sections below. New job quickstart","title":"Manage jobs"},{"location":"compute/jobs/#new-job-from-scratch","text":"In addition to the options available with the new job quickstart, from this menu you can set default arguments to be given as input to the job when executed. You can overwrite these arguments when running a job by providing input arguments for that particular execution of the job. For example, in the image below the default arguments \"a b c\" will be passed as space-separated arguments to the job. Job default arguments","title":"New job from scratch"},{"location":"compute/jobs/#import-an-existing-job","text":"Instead of creating a new job from scratch, you can create one based on another jobs' configuration. The latter can be exported by using the Export button from the job's Overview page as shown below. This action will download a .json file containing the job's specification which you can then import from the advanced options page. Job export","title":"Import an existing job"},{"location":"compute/jobs/#compute-configuration","text":"You can set a default configuration for each job type (currently only supported for Spark and Python ) at a project scope, from the Project settings - Compute configuration menu option. That enables you to have the job configuration parameters already filled in when creating a job and optionally override them for this particular job instance. Compute configuration page","title":"Compute Configuration"},{"location":"compute/jobs/#spark","text":"The image below shows an example of a Spark job advanced configuration page. If set, these configuration properties will override the default project-scoped job configuration properties described in the section above. You can set the following properties for Spark/PySpark jobs: Main class: For Spark jobs, the main class of the application Driver memory Driver virtual cores Executor memory Executor virtual cores Dynamic - Static: Run the Spark job in static or dynamic allocation mode (see official docs for details). Additional archives: List of zip or .tgz files that will be locally accessible by the application Additional jars: List of .jar files to add to the CLASSPATH of the application Additional Python dependencies: List of .py, .zip or .egg files that will be locally accessible by the application Additional files: List of files that will be locally accessible by the application Properties: Optional line-separates properties to be set for the Spark application. For example, to set environment variables for the driver you can set the properties as shown below spark.yarn.appMasterEnv.envvar=value spark.yarn.appMasterEnv.envvar2=value2 Spark job configuration properties","title":"Spark"},{"location":"compute/jobs/#python","text":"The image below shows an example of a Python job advanced configuration page. If set, these configuration properties will override the default project-scoped job configuration properties described in the section above. You can set the following properties for Python jobs: Container memory: The amount of memory in MB to be allocated to the container running the Python program Container cores: The number of cores to be allocated to the container running the Python program Additional files: List of files that will be locally accessible by the application Python job configuration properties You do not have to upload the Python program via the Hopsworks UI to run it. It can be done programmatically from a Python program by using the upload function of the dataset module of the hops Python library To do that, first generate an API key for your project, and then use the project.connect() function of the same library to connect to a project of your Hopsworks cluster and then dataset.upload() .","title":"Python"},{"location":"compute/jobs/#docker","text":"Docker jobs can currently only be managed from the legacy Hopsworks user interface, see documentation at Docker jobs for details.","title":"Docker"},{"location":"compute/jobs/#apache-flink","text":"Flink jobs can currently only be managed from the legacy Hopsworks user interface, see documentation at Flink jobs for details.","title":"Apache Flink"},{"location":"compute/jobs/#alerts","text":"You can attach Hopsworks alerts on a per-job basis if you want to receive notifications for job-specific events. Firstly, a Hopsworks administrator needs to setup alert channels from the Cluster settings page. Currently supported channels are - email - slack - pagerduty Once you have configured the alert channels, you can proceed by specifying alert receivers from the project's settings page. Receivers are the destinations to which your alerts will be sent to. You can specify global alerts, which means these alerts will be triggered on job events regardless of which specific job generated the alert event. Alternatively, you can override the job alerts by adding alerts to a specific job from the job's configuration page as shown below. The job events alerts are triggered upon are: on job success on job fail on job killed The potential severities these alerts should report are: WARNING INFO CRITICAL Job alerts","title":"Alerts"},{"location":"compute/jobs/#edit-delete","text":"You can edit a job from the Job's overview page. If you would like to quickly create a job based on the configuration of another job, you can use the Make a copy button that will prompt you for a name for the new job and then immediately after create the new job. From the same page, you can also delete a job as shown in the image below. Please note that when deleting a job, Hopsworks will first attempt to gracefully terminate any pending executions of this job. All job logs will remain in the Logs dataset and it is up to the user to clean them up if needed. Job deletion","title":"Edit &amp; Delete"},{"location":"compute/jobs/#executions","text":"You can execute a job multiple times concurrently with different input arguments. Each execution has an individual execution id that identifies it. Execution logs are stored under the Logs dataset and are categorized per job type, name, execution(Python)/application(Spark) id.","title":"Executions"},{"location":"compute/jobs/#execute-a-job","text":"You can execute a job either using the Quick run option from the job's preview page, as shown in the image below, or from the job's overview page. The difference between the two is that the latter will prompt you for input arguments whereas the former will use the default arguments, if any are provided. This page also shows a list of the most recent executions. For a detailed list of executions for the job, you need navigate to the executions page by clicking the view all executions link. Job preview Executions overview By default, a job can have a maximum of 10000 executions. This limit can be increased/lowered by a Hopsworks administrator.","title":"Execute a job"},{"location":"compute/jobs/#monitoring-and-logging","text":"The executions page enables you to filter executions for the job based on the execution's date and state. You can view detailed information about each execution by following the various monitoring and logging links provided with each execution. In particular, you get access to the following dashboards/views for every execution: Spark UI: Applies to Spark/PySpark jobs only, a new tab will open showing the Spark application Web UI (see docs ). RM UI: Applies to Spark/PySpark/Flink jobs only, a new tab will open showing the Apache Hadoop YARN page of the execution Kibana: Applies to Spark/PySpark/Flink jobs only, a new tab will open showing the execution logs in real-time as they are collected from all the containers the execution is distributed at. Logs: Shows the aggregated stdout/stderr logs of the execution. These logs are stored under the Logs dataset.","title":"Monitoring and Logging"},{"location":"compute/jobs/#hopsworks-ide-plugin","text":"It is also possible to work on jobs while developing in your IntelliJ/PyCharm IDE by installing the Hopsworks Plugin from the marketplace. Usage Open the Hopsworks Job Preferences UI for specifying user preferences under Settings -> Tools -> Hopsworks Job Preferences . Input the Hopsworks project preferences and job details you wish to work on. Open a Project and within the Project Explorer, right click on the program ( .jar, .py, .ipynb) you wish to execute as a job on Hopsworks. Different job actions possible are available in the context menu ( Create, Run, Stop, etc.) Actions Create: Create or update job as specified in Hopsworks Job Preferences Run: Uploads the program first to the HDFS path as specified and runs job Stop: Stops a job Delete: Deletes a job Job Execution Status / Job Execution Logs: Get the job status or logs respectively. You have the option of retrieving a particular job execution by specifying the execution id in the 'Hopsworks Job Preferences' UI, otherwise default is the last execution for the job name specified. Working with jobs from Hopsworks IntelliJ/PyCharm plugin","title":"Hopsworks IDE Plugin"},{"location":"compute/jupyter/","text":"Jupyter notebooks # Developing with Jupyter notebooks is provided as a service in Hopsworks as part of the compute section of a project which you can access from the main project menu. The image below shows the Jupyter service page in Hopsworks. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below. Configuration and start # Hopsworks supports both JupyterLab and classic Jupyter as Jupyter development frameworks. Clicking Run Jupyter , will start JupyterLab by default. You can then open classic Jupyter from within JupyterLab by navigating to Help - Launch Classic Notebook . Hopsworks will attempt to open JupyterLab in a new browser tab. You can also click the Open Jupyter button to open Jupyter in a new tab if your browser is blocking this operation by default. Classic Jupyter Jupyter server is using the same configuration options as the Jobs service to run notebooks. Currently, the Spark/PySpark and Python configuration options are supported. For more information on how to set these options, see the Jobs section and for more information on how to work with Spark and Jupyter in Hopsworks see the Jupyter + Spark on Hopsworks below. If you would like to switch between configurations, you will have to shut down the running Jupyter server first. After starting a Spark/PySpark notebook, you can view monitoring and logging information as shown in the image below. For Spark/PySpark notebooks, this information is identical to the information provided when running Spark/PySpark jobs from the Jobs dashboard (see Jobs section for details). You can start multiple notebooks from the same Jupyter server, however keep in mind that for each notebook new compute resources will be allocated based on the configuration options Jupyter was started with. Jupyter dashboard page showing currently running applications Recent notebooks # When you run a notebook, its Jupyter configuration is stored and attached to the notebook. You can use this configuration later to start the Jupyter notebook directly from the Jupyter dashboard by clicking Open in Jupyter under the Recent notebooks section of the dashboard. For example, in the image above you can see a notebook called hello_world.ipynb which you can start directly. Hopsworks will use the Jupyter server configuration the notebook ran with last time. Logs # It can be useful to look at the Jupyter server logs in case of errors as they can provide more details compared to the error notification that is shown in the Jupyter dashboard. For example if Jupyter cannot start, simply click the Server Logs button next to the Start or Stop button in the Jupyter dashboard. This will open a new tab ( make sure your browser does not block the new tab! ) with the Jupyter logs as shown in the figure below. Jupyter logs Auto-shutdown # Jupyter server is configured to automatically shut down after 6 hours. This functionality enables you to avoid excessive use of compute resources. You can modify the shut down limit by adding 6-hour chunks to the shutdown time. The dashboard also shows you how much time is currently left before the Jupyter server shuts down. Debug Jupyter installation # Jupyter is installed in the Python environment of your project. This means that if a dependency of Jupyter is removed, or an incorrect version is installed it may not work properly. If the Python environment ends up in a state with conflicting libraries installed then an alert will be shown in the Python libraries page under the Project settings menu. Jupyter + Spark on Hopsworks # As a user, you will just interact with the Jupyter notebooks, but below you can find a detailed explanation of the technology behind the scenes. When using Jupyter on Hopsworks, a library called sparkmagic _ is used to interact with the Hopsworks cluster. When you create a Jupyter notebook on Hopsworks, you first select a kernel . A kernel is simply a program that executes the code that you have in the Jupyter cells, you can think of it as a REPL-backend to your jupyter notebook that acts as a frontend. Sparkmagic works with a remote REST server for Spark, called livy , running inside the Hopsworks cluster. Livy is an interface that Jupyter-on-Hopsworks uses to interact with the cluster. When you run Jupyter cells using the pyspark kernel, the kernel will automatically send commands to livy in the background for executing the commands on the cluster. Thus, the work that happens in the background when you run a Jupyter cell is as follows: The code in the cell will first go to the kernel. Next, the kernel sends the code as an HTTP REST request to Livy. When receiving the REST request, Livy executes the code on the Spark driver in the cluster. If the code is regular python/scala/R code, it will run inside a python/scala/R interpreter on the Spark driver. If the code includes a spark command, using the spark session, a spark job will be launched on the cluster from the Spark driver. When the python/scala/R or spark execution is finished, the results are sent back from Livy to the pyspark kernel/sparkmagic. Finally, the pyspark kernel displays the result in the Jupyter notebook. The three Jupyter kernels we support on Hopsworks are: Spark, a kernel for executing scala code and interacting with the cluster through spark-scala PySpark, a kernel for executing python code and interacting with the cluster through pyspark SparkR, a kernel for executing R code and interacting with the cluster through spark-R By default, all files and folders created by Spark are group writable (i.e umask=007). If you want to change this default umask you can add additional spark property spark.hadoop.fs.permissions.umask-mode=<umask> in the Properties textbox of the Jupyter server configuration, before starting the jupyter server. In the rest of this tutorial we will focus on the pyspark kernel. PySpark notebooks # After you have started the Jupyter notebook server, you can create a PySpark notebook from JupyterLab: Create a pyspark notebook When you execute the first cell in a PySpark notebook, the Spark session is automatically created, referring to the Hopsworks cluster. SparkSession creation with pyspark kernel The notebook will look just like any Python notebook, with the difference that the python interpreter is actually running on a Spark driver in the cluster. You can execute regular Python code: Executing python code on the spark driver in the cluster Since you are executing on the Spark driver, you can also run applications on spark executors in the cluster, the Spark session is available as the variable spark in the notebook: Starting a spark application from Jupyter When you execute a cell in Jupyter that starts a Spark job, you can go back to the Hopsworks Jupyter service dashboard where you can access the Spark web UI and other monitoring and logging tools. In addition to having access to a regular Python interpreter as well as the spark cluster, you also have access to magic commands provided by sparkmagic. You can view a list of all commands by executing a cell with %%help : Printing a list of all sparkmagic commands Plotting with PySpark Kernel # So far throughout this tutorial, the Jupyter notebook have behaved more or less identical to how it does if you start the notebook server locally on your machine using a Python kernel, without access to a Hopsworks cluster. However, there is one main difference from a user-standpoint when using PySpark notebooks instead of regular Python notebooks, this is related to plotting. Since the code in a PySpark notebook is executed remotely, in the Spark cluster, regular Python plotting will not work. What you can do however, is to use sparkmagic to download your remote spark dataframe as a local pandas dataframe and plot it using matplotlib , seaborn , or sparkmagic 's built-in visualization. To achieve this we use the magics: %%sql , %%spark , and %%local . The steps to do plotting using a PySpark notebook are illustrated below. Using this approach, you can have large scale cluster computation and plotting in the same notebook. Step 1 : Create a remote Spark Dataframe: Creating a spark dataframe Step 2 : Download the Spark Dataframe to a local Pandas Dataframe using %%sql or %%spark: Note : you should not try to download large spark dataframes for plotting. When you plot a dataframe, the entire dataframe must fit into memory, so add the flag \u2013maxrows x to limit the dataframe size when you download it to the local Jupyter server for plotting. Using %%sql : Downloading the spark dataframe to a pandas dataframe using %%sql Using %%spark : Downloading the spark dataframe to a pandas dataframe using %%spark Step 3 : Plot the pandas dataframe using Python plotting libraries: When you download a dataframe from Spark to Pandas with sparkmagic, it gives you a default visualization of the data using autovizwidget , as you saw in the screenshots above. However, sometimes you want custom plots, using matplotlib or seaborn. To do this, use the sparkmagic %%local to access the local pandas dataframe and then you can plot as usual. Import plotting libraries locally on the Jupyter server Plot a local pandas dataframe using seaborn and the magic %%local Plot a local pandas dataframe using matplotlib and the magic %%local Want to Learn More? # We provide a large number of example notebooks available at examples.hopsworks.ai with the notebook files themselves being hosted at the Hopsworks examples GitHub repository . Go to Hopsworks and try them out!","title":"Jupyter"},{"location":"compute/jupyter/#jupyter-notebooks","text":"Developing with Jupyter notebooks is provided as a service in Hopsworks as part of the compute section of a project which you can access from the main project menu. The image below shows the Jupyter service page in Hopsworks. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below.","title":"Jupyter notebooks"},{"location":"compute/jupyter/#configuration-and-start","text":"Hopsworks supports both JupyterLab and classic Jupyter as Jupyter development frameworks. Clicking Run Jupyter , will start JupyterLab by default. You can then open classic Jupyter from within JupyterLab by navigating to Help - Launch Classic Notebook . Hopsworks will attempt to open JupyterLab in a new browser tab. You can also click the Open Jupyter button to open Jupyter in a new tab if your browser is blocking this operation by default. Classic Jupyter Jupyter server is using the same configuration options as the Jobs service to run notebooks. Currently, the Spark/PySpark and Python configuration options are supported. For more information on how to set these options, see the Jobs section and for more information on how to work with Spark and Jupyter in Hopsworks see the Jupyter + Spark on Hopsworks below. If you would like to switch between configurations, you will have to shut down the running Jupyter server first. After starting a Spark/PySpark notebook, you can view monitoring and logging information as shown in the image below. For Spark/PySpark notebooks, this information is identical to the information provided when running Spark/PySpark jobs from the Jobs dashboard (see Jobs section for details). You can start multiple notebooks from the same Jupyter server, however keep in mind that for each notebook new compute resources will be allocated based on the configuration options Jupyter was started with. Jupyter dashboard page showing currently running applications","title":"Configuration and start"},{"location":"compute/jupyter/#recent-notebooks","text":"When you run a notebook, its Jupyter configuration is stored and attached to the notebook. You can use this configuration later to start the Jupyter notebook directly from the Jupyter dashboard by clicking Open in Jupyter under the Recent notebooks section of the dashboard. For example, in the image above you can see a notebook called hello_world.ipynb which you can start directly. Hopsworks will use the Jupyter server configuration the notebook ran with last time.","title":"Recent notebooks"},{"location":"compute/jupyter/#logs","text":"It can be useful to look at the Jupyter server logs in case of errors as they can provide more details compared to the error notification that is shown in the Jupyter dashboard. For example if Jupyter cannot start, simply click the Server Logs button next to the Start or Stop button in the Jupyter dashboard. This will open a new tab ( make sure your browser does not block the new tab! ) with the Jupyter logs as shown in the figure below. Jupyter logs","title":"Logs"},{"location":"compute/jupyter/#auto-shutdown","text":"Jupyter server is configured to automatically shut down after 6 hours. This functionality enables you to avoid excessive use of compute resources. You can modify the shut down limit by adding 6-hour chunks to the shutdown time. The dashboard also shows you how much time is currently left before the Jupyter server shuts down.","title":"Auto-shutdown"},{"location":"compute/jupyter/#debug-jupyter-installation","text":"Jupyter is installed in the Python environment of your project. This means that if a dependency of Jupyter is removed, or an incorrect version is installed it may not work properly. If the Python environment ends up in a state with conflicting libraries installed then an alert will be shown in the Python libraries page under the Project settings menu.","title":"Debug Jupyter installation"},{"location":"compute/jupyter/#jupyter-spark-on-hopsworks","text":"As a user, you will just interact with the Jupyter notebooks, but below you can find a detailed explanation of the technology behind the scenes. When using Jupyter on Hopsworks, a library called sparkmagic _ is used to interact with the Hopsworks cluster. When you create a Jupyter notebook on Hopsworks, you first select a kernel . A kernel is simply a program that executes the code that you have in the Jupyter cells, you can think of it as a REPL-backend to your jupyter notebook that acts as a frontend. Sparkmagic works with a remote REST server for Spark, called livy , running inside the Hopsworks cluster. Livy is an interface that Jupyter-on-Hopsworks uses to interact with the cluster. When you run Jupyter cells using the pyspark kernel, the kernel will automatically send commands to livy in the background for executing the commands on the cluster. Thus, the work that happens in the background when you run a Jupyter cell is as follows: The code in the cell will first go to the kernel. Next, the kernel sends the code as an HTTP REST request to Livy. When receiving the REST request, Livy executes the code on the Spark driver in the cluster. If the code is regular python/scala/R code, it will run inside a python/scala/R interpreter on the Spark driver. If the code includes a spark command, using the spark session, a spark job will be launched on the cluster from the Spark driver. When the python/scala/R or spark execution is finished, the results are sent back from Livy to the pyspark kernel/sparkmagic. Finally, the pyspark kernel displays the result in the Jupyter notebook. The three Jupyter kernels we support on Hopsworks are: Spark, a kernel for executing scala code and interacting with the cluster through spark-scala PySpark, a kernel for executing python code and interacting with the cluster through pyspark SparkR, a kernel for executing R code and interacting with the cluster through spark-R By default, all files and folders created by Spark are group writable (i.e umask=007). If you want to change this default umask you can add additional spark property spark.hadoop.fs.permissions.umask-mode=<umask> in the Properties textbox of the Jupyter server configuration, before starting the jupyter server. In the rest of this tutorial we will focus on the pyspark kernel.","title":"Jupyter + Spark on Hopsworks"},{"location":"compute/jupyter/#pyspark-notebooks","text":"After you have started the Jupyter notebook server, you can create a PySpark notebook from JupyterLab: Create a pyspark notebook When you execute the first cell in a PySpark notebook, the Spark session is automatically created, referring to the Hopsworks cluster. SparkSession creation with pyspark kernel The notebook will look just like any Python notebook, with the difference that the python interpreter is actually running on a Spark driver in the cluster. You can execute regular Python code: Executing python code on the spark driver in the cluster Since you are executing on the Spark driver, you can also run applications on spark executors in the cluster, the Spark session is available as the variable spark in the notebook: Starting a spark application from Jupyter When you execute a cell in Jupyter that starts a Spark job, you can go back to the Hopsworks Jupyter service dashboard where you can access the Spark web UI and other monitoring and logging tools. In addition to having access to a regular Python interpreter as well as the spark cluster, you also have access to magic commands provided by sparkmagic. You can view a list of all commands by executing a cell with %%help : Printing a list of all sparkmagic commands","title":"PySpark notebooks"},{"location":"compute/jupyter/#plotting-with-pyspark-kernel","text":"So far throughout this tutorial, the Jupyter notebook have behaved more or less identical to how it does if you start the notebook server locally on your machine using a Python kernel, without access to a Hopsworks cluster. However, there is one main difference from a user-standpoint when using PySpark notebooks instead of regular Python notebooks, this is related to plotting. Since the code in a PySpark notebook is executed remotely, in the Spark cluster, regular Python plotting will not work. What you can do however, is to use sparkmagic to download your remote spark dataframe as a local pandas dataframe and plot it using matplotlib , seaborn , or sparkmagic 's built-in visualization. To achieve this we use the magics: %%sql , %%spark , and %%local . The steps to do plotting using a PySpark notebook are illustrated below. Using this approach, you can have large scale cluster computation and plotting in the same notebook. Step 1 : Create a remote Spark Dataframe: Creating a spark dataframe Step 2 : Download the Spark Dataframe to a local Pandas Dataframe using %%sql or %%spark: Note : you should not try to download large spark dataframes for plotting. When you plot a dataframe, the entire dataframe must fit into memory, so add the flag \u2013maxrows x to limit the dataframe size when you download it to the local Jupyter server for plotting. Using %%sql : Downloading the spark dataframe to a pandas dataframe using %%sql Using %%spark : Downloading the spark dataframe to a pandas dataframe using %%spark Step 3 : Plot the pandas dataframe using Python plotting libraries: When you download a dataframe from Spark to Pandas with sparkmagic, it gives you a default visualization of the data using autovizwidget , as you saw in the screenshots above. However, sometimes you want custom plots, using matplotlib or seaborn. To do this, use the sparkmagic %%local to access the local pandas dataframe and then you can plot as usual. Import plotting libraries locally on the Jupyter server Plot a local pandas dataframe using seaborn and the magic %%local Plot a local pandas dataframe using matplotlib and the magic %%local","title":"Plotting with PySpark Kernel"},{"location":"compute/jupyter/#want-to-learn-more","text":"We provide a large number of example notebooks available at examples.hopsworks.ai with the notebook files themselves being hosted at the Hopsworks examples GitHub repository . Go to Hopsworks and try them out!","title":"Want to Learn More?"},{"location":"compute/python/","text":"Python environment # Hopsworks comes out of the box with a Python environment for data engineering, machine learning and more general data science development. There is one Python environment for each project. A library installed in the project's Python environment can be used in a Job or Jupyter notebook (Python or PySpark) run in the project. The environment ensures compatibility between the CUDA version and the installed TensorFlow and PyTorch versions for applications using NVIDIA GPUs. Navigating to the service # Managing the project's Python environment is provided as a service on Hopsworks and can be found in the Python libraries page under the Project settings menu. Open the Python libraries page When a project is created, the python 3.8 environment is automatically enabled. So no additional steps are required to start developing your machine learning application. Listing installed libraries # The list of installed libraries is displayed on the page. It is possible to filter based on the name and package manager to search for a particular library. Viewing installed libraries Installing libraries # Python packages can be installed from the following sources: PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip Install by name and version # Enter the name and, optionally, the desired version to install. Installing library by name and version Search and install # Enter the search term and select a library and version to install. Installing library using search Install from package file # Install a python package by uploading the corresponding package file and selecting it in the file browser. Currently supported formats include: .whl .egg requirements.txt environment.yml Installing library from file Install from .git repository # To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo Uninstalling libraries # To uninstall a library, find the library in the list of installed libararies tab and click the trash icon to remove the library. Uninstall a library Debugging the environment # After each installation or uninstall of a library, the environment is analyzed to detect libraries that may not work properly. In order to do so we use the pip check tool, which is able to identify missing dependencies or if a dependency is installed with the incorrect version. The alert will automatically show if such an issue was found. Conflicts detected in environment Exporting an environment # An existing Anaconda environment can be exported as a yml file, clicking the Export env will download the environment.yml file in your browser. Export environment Recreating the environment # Sometimes it may be desirable to recreate the environment to start from the default project environment. In order to do that, first click Remove env . Remove environment After removing the environment, simply recreate it by clicking Create Environment . Create environment","title":"Python"},{"location":"compute/python/#python-environment","text":"Hopsworks comes out of the box with a Python environment for data engineering, machine learning and more general data science development. There is one Python environment for each project. A library installed in the project's Python environment can be used in a Job or Jupyter notebook (Python or PySpark) run in the project. The environment ensures compatibility between the CUDA version and the installed TensorFlow and PyTorch versions for applications using NVIDIA GPUs.","title":"Python environment"},{"location":"compute/python/#navigating-to-the-service","text":"Managing the project's Python environment is provided as a service on Hopsworks and can be found in the Python libraries page under the Project settings menu. Open the Python libraries page When a project is created, the python 3.8 environment is automatically enabled. So no additional steps are required to start developing your machine learning application.","title":"Navigating to the service"},{"location":"compute/python/#listing-installed-libraries","text":"The list of installed libraries is displayed on the page. It is possible to filter based on the name and package manager to search for a particular library. Viewing installed libraries","title":"Listing installed libraries"},{"location":"compute/python/#installing-libraries","text":"Python packages can be installed from the following sources: PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip","title":"Installing libraries"},{"location":"compute/python/#install-by-name-and-version","text":"Enter the name and, optionally, the desired version to install. Installing library by name and version","title":"Install by name and version"},{"location":"compute/python/#search-and-install","text":"Enter the search term and select a library and version to install. Installing library using search","title":"Search and install"},{"location":"compute/python/#install-from-package-file","text":"Install a python package by uploading the corresponding package file and selecting it in the file browser. Currently supported formats include: .whl .egg requirements.txt environment.yml Installing library from file","title":"Install from package file"},{"location":"compute/python/#install-from-git-repository","text":"To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo","title":"Install from .git repository"},{"location":"compute/python/#uninstalling-libraries","text":"To uninstall a library, find the library in the list of installed libararies tab and click the trash icon to remove the library. Uninstall a library","title":"Uninstalling libraries"},{"location":"compute/python/#debugging-the-environment","text":"After each installation or uninstall of a library, the environment is analyzed to detect libraries that may not work properly. In order to do so we use the pip check tool, which is able to identify missing dependencies or if a dependency is installed with the incorrect version. The alert will automatically show if such an issue was found. Conflicts detected in environment","title":"Debugging the environment"},{"location":"compute/python/#exporting-an-environment","text":"An existing Anaconda environment can be exported as a yml file, clicking the Export env will download the environment.yml file in your browser. Export environment","title":"Exporting an environment"},{"location":"compute/python/#recreating-the-environment","text":"Sometimes it may be desirable to recreate the environment to start from the default project environment. In order to do that, first click Remove env . Remove environment After removing the environment, simply recreate it by clicking Create Environment . Create environment","title":"Recreating the environment"}]}